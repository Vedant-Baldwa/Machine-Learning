{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f26310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b8f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using :  cuda\n"
     ]
    }
   ],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using : \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dfaa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train=transforms.Compose([\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(0,translate=(0.1,0.1)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_test=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainset=torchvision.datasets.CIFAR100(root=\"./data\",train=True,download=True,transform=transform_train)\n",
    "testset=torchvision.datasets.CIFAR100(root=\"./data\",train=False,download=True,transform=transform_test)\n",
    "\n",
    "train_idx,val_idx=train_test_split(\n",
    "    np.arange(len(trainset)),test_size=0.1,stratify=trainset.targets,random_state=24\n",
    ")\n",
    "\n",
    "train_subset=Subset(trainset,train_idx)\n",
    "val_subset=Subset(trainset,val_idx)\n",
    "\n",
    "\n",
    "batch_size=128\n",
    "trainloader=DataLoader(train_subset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "valloader=DataLoader(val_subset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "testloader=DataLoader(testset,batch_size=batch_size,shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d12235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Dropout(p=0.3, inplace=False)\n",
      "    (16): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): ReLU()\n",
      "    (19): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (21): ReLU()\n",
      "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (23): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=1)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=128, out_features=512, bias=False)\n",
      "    (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=512, out_features=100, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self,num_classes=100):\n",
    "        super().__init__()\n",
    "        self.features=nn.Sequential(\n",
    "\n",
    "            #Block 1\n",
    "            nn.Conv2d(3,32,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,32,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            #Block 2\n",
    "            nn.Conv2d(32,64,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            #Block 3\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1,bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.4),\n",
    "        )\n",
    "\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128,512,bias=False),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512,num_classes)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        x=self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model=CustomCNN(num_classes=100).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f78d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "schedular=optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode=\"min\",factor=0.5,patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b3e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/60\n",
      "Train loss 3.9143 acc 0.091 | Val loss 3.5375 acc 0.138\n",
      "Saved best Model\n",
      "\n",
      "Epoch 2/60\n",
      "Train loss 3.4336 acc 0.163 | Val loss 3.2662 acc 0.195\n",
      "Saved best Model\n",
      "\n",
      "Epoch 3/60\n",
      "Train loss 3.1496 acc 0.213 | Val loss 3.0743 acc 0.225\n",
      "Saved best Model\n",
      "\n",
      "Epoch 4/60\n",
      "Train loss 2.9374 acc 0.252 | Val loss 2.9959 acc 0.246\n",
      "Saved best Model\n",
      "\n",
      "Epoch 5/60\n",
      "Train loss 2.7858 acc 0.282 | Val loss 2.8970 acc 0.270\n",
      "Saved best Model\n",
      "\n",
      "Epoch 6/60\n",
      "Train loss 2.6657 acc 0.310 | Val loss 2.5015 acc 0.342\n",
      "Saved best Model\n",
      "\n",
      "Epoch 7/60\n",
      "Train loss 2.5680 acc 0.327 | Val loss 2.5203 acc 0.339\n",
      "\n",
      "Epoch 8/60\n",
      "Train loss 2.4853 acc 0.346 | Val loss 2.3737 acc 0.376\n",
      "Saved best Model\n",
      "\n",
      "Epoch 9/60\n",
      "Train loss 2.4263 acc 0.360 | Val loss 2.3739 acc 0.376\n",
      "\n",
      "Epoch 10/60\n",
      "Train loss 2.3635 acc 0.372 | Val loss 2.2300 acc 0.396\n",
      "Saved best Model\n",
      "\n",
      "Epoch 11/60\n",
      "Train loss 2.3155 acc 0.383 | Val loss 2.2033 acc 0.405\n",
      "Saved best Model\n",
      "\n",
      "Epoch 12/60\n",
      "Train loss 2.2562 acc 0.395 | Val loss 2.1019 acc 0.424\n",
      "Saved best Model\n",
      "\n",
      "Epoch 13/60\n",
      "Train loss 2.2084 acc 0.405 | Val loss 2.2693 acc 0.400\n",
      "\n",
      "Epoch 14/60\n",
      "Train loss 2.1687 acc 0.416 | Val loss 2.0660 acc 0.439\n",
      "Saved best Model\n",
      "\n",
      "Epoch 15/60\n",
      "Train loss 2.1379 acc 0.424 | Val loss 2.0563 acc 0.438\n",
      "Saved best Model\n",
      "\n",
      "Epoch 16/60\n",
      "Train loss 2.0936 acc 0.434 | Val loss 2.1010 acc 0.434\n",
      "\n",
      "Epoch 17/60\n",
      "Train loss 2.0743 acc 0.440 | Val loss 1.9814 acc 0.459\n",
      "Saved best Model\n",
      "\n",
      "Epoch 18/60\n",
      "Train loss 2.0494 acc 0.442 | Val loss 2.0300 acc 0.446\n",
      "\n",
      "Epoch 19/60\n",
      "Train loss 2.0268 acc 0.448 | Val loss 2.0639 acc 0.444\n",
      "\n",
      "Epoch 20/60\n",
      "Train loss 1.9921 acc 0.454 | Val loss 1.9812 acc 0.465\n",
      "Saved best Model\n",
      "\n",
      "Epoch 21/60\n",
      "Train loss 1.9791 acc 0.457 | Val loss 1.9017 acc 0.484\n",
      "Saved best Model\n",
      "\n",
      "Epoch 22/60\n",
      "Train loss 1.9554 acc 0.465 | Val loss 1.8821 acc 0.476\n",
      "Saved best Model\n",
      "\n",
      "Epoch 23/60\n",
      "Train loss 1.9342 acc 0.469 | Val loss 1.8432 acc 0.492\n",
      "Saved best Model\n",
      "\n",
      "Epoch 24/60\n",
      "Train loss 1.9147 acc 0.475 | Val loss 1.7943 acc 0.500\n",
      "Saved best Model\n",
      "\n",
      "Epoch 25/60\n",
      "Train loss 1.8994 acc 0.477 | Val loss 1.7994 acc 0.502\n",
      "\n",
      "Epoch 26/60\n",
      "Train loss 1.8821 acc 0.484 | Val loss 1.8265 acc 0.496\n",
      "\n",
      "Epoch 27/60\n",
      "Train loss 1.8750 acc 0.484 | Val loss 1.9120 acc 0.477\n",
      "\n",
      "Epoch 28/60\n",
      "Train loss 1.8553 acc 0.490 | Val loss 1.8508 acc 0.504\n",
      "\n",
      "Epoch 29/60\n",
      "Train loss 1.7681 acc 0.507 | Val loss 1.6875 acc 0.527\n",
      "Saved best Model\n",
      "\n",
      "Epoch 30/60\n",
      "Train loss 1.7503 acc 0.512 | Val loss 1.6894 acc 0.534\n",
      "\n",
      "Epoch 31/60\n",
      "Train loss 1.7424 acc 0.513 | Val loss 1.7164 acc 0.523\n",
      "\n",
      "Epoch 32/60\n",
      "Train loss 1.7288 acc 0.520 | Val loss 1.7050 acc 0.525\n",
      "\n",
      "Epoch 33/60\n",
      "Train loss 1.7168 acc 0.522 | Val loss 1.6506 acc 0.540\n",
      "Saved best Model\n",
      "\n",
      "Epoch 34/60\n",
      "Train loss 1.7089 acc 0.522 | Val loss 1.6655 acc 0.535\n",
      "\n",
      "Epoch 35/60\n",
      "Train loss 1.7019 acc 0.525 | Val loss 1.6667 acc 0.535\n",
      "\n",
      "Epoch 36/60\n",
      "Train loss 1.6922 acc 0.527 | Val loss 1.6580 acc 0.539\n",
      "\n",
      "Epoch 37/60\n",
      "Train loss 1.6902 acc 0.527 | Val loss 1.6378 acc 0.545\n",
      "Saved best Model\n",
      "\n",
      "Epoch 38/60\n",
      "Train loss 1.6729 acc 0.533 | Val loss 1.6468 acc 0.543\n",
      "\n",
      "Epoch 39/60\n",
      "Train loss 1.6760 acc 0.531 | Val loss 1.6468 acc 0.541\n",
      "\n",
      "Epoch 40/60\n",
      "Train loss 1.6636 acc 0.535 | Val loss 1.6546 acc 0.537\n",
      "\n",
      "Epoch 41/60\n",
      "Train loss 1.6591 acc 0.535 | Val loss 1.6134 acc 0.551\n",
      "Saved best Model\n",
      "\n",
      "Epoch 42/60\n",
      "Train loss 1.6567 acc 0.536 | Val loss 1.6005 acc 0.552\n",
      "Saved best Model\n",
      "\n",
      "Epoch 43/60\n",
      "Train loss 1.6399 acc 0.540 | Val loss 1.6371 acc 0.547\n",
      "\n",
      "Epoch 44/60\n",
      "Train loss 1.6500 acc 0.537 | Val loss 1.6243 acc 0.550\n",
      "\n",
      "Epoch 45/60\n",
      "Train loss 1.6377 acc 0.539 | Val loss 1.6052 acc 0.558\n",
      "\n",
      "Epoch 46/60\n",
      "Train loss 1.6339 acc 0.541 | Val loss 1.6090 acc 0.550\n",
      "\n",
      "Epoch 47/60\n",
      "Train loss 1.5922 acc 0.553 | Val loss 1.5561 acc 0.570\n",
      "Saved best Model\n",
      "\n",
      "Epoch 48/60\n",
      "Train loss 1.5825 acc 0.554 | Val loss 1.6174 acc 0.552\n",
      "\n",
      "Epoch 49/60\n",
      "Train loss 1.5761 acc 0.554 | Val loss 1.5870 acc 0.557\n",
      "\n",
      "Epoch 50/60\n",
      "Train loss 1.5759 acc 0.556 | Val loss 1.5729 acc 0.558\n",
      "\n",
      "Epoch 51/60\n",
      "Train loss 1.5617 acc 0.557 | Val loss 1.5704 acc 0.562\n",
      "\n",
      "Epoch 52/60\n",
      "Train loss 1.5510 acc 0.560 | Val loss 1.5402 acc 0.567\n",
      "Saved best Model\n",
      "\n",
      "Epoch 53/60\n",
      "Train loss 1.5404 acc 0.563 | Val loss 1.5409 acc 0.569\n",
      "\n",
      "Epoch 54/60\n",
      "Train loss 1.5365 acc 0.563 | Val loss 1.5459 acc 0.571\n",
      "\n",
      "Epoch 55/60\n",
      "Train loss 1.5303 acc 0.567 | Val loss 1.5271 acc 0.565\n",
      "Saved best Model\n",
      "\n",
      "Epoch 56/60\n",
      "Train loss 1.5311 acc 0.564 | Val loss 1.5391 acc 0.573\n",
      "\n",
      "Epoch 57/60\n",
      "Train loss 1.5303 acc 0.565 | Val loss 1.5382 acc 0.570\n",
      "\n",
      "Epoch 58/60\n",
      "Train loss 1.5292 acc 0.566 | Val loss 1.5314 acc 0.569\n",
      "\n",
      "Epoch 59/60\n",
      "Train loss 1.5288 acc 0.565 | Val loss 1.5421 acc 0.569\n",
      "\n",
      "Epoch 60/60\n",
      "Train loss 1.5139 acc 0.571 | Val loss 1.5282 acc 0.574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=60\n",
    "patience=8\n",
    "best_model_wts=copy.deepcopy(model.state_dict())\n",
    "best_val_loss=float(\"inf\")\n",
    "early_stop_counter=0\n",
    "\n",
    "train_losss,val_losss,train_accs,val_accs=[],[],[],[]\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    running_loss,correct,total=0.0,0,0\n",
    "    for imgs,labels in trainloader:\n",
    "        imgs,labels=imgs.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(imgs)\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item() * imgs.size(0)\n",
    "        _,preds=outputs.max(1)\n",
    "        correct+=preds.eq(labels).sum().item()\n",
    "        total+=labels.size(0)\n",
    "    \n",
    "    trian_loss=running_loss/total\n",
    "    train_acc=correct/total\n",
    "    train_losss.append(trian_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss,val_correct,val_total=0.0,0,0\n",
    "    with torch.no_grad():\n",
    "        for imgs,labels in valloader:\n",
    "            imgs,labels=imgs.to(device),labels.to(device)\n",
    "            outputs=model(imgs)\n",
    "            loss=criterion(outputs,labels)\n",
    "            val_loss+=loss.item() * imgs.size(0)\n",
    "            _,preds=outputs.max(1)\n",
    "            val_correct+=preds.eq(labels).sum().item()\n",
    "            val_total+=labels.size(0)\n",
    "        \n",
    "    val_loss/=val_total\n",
    "    val_acc=val_correct/val_total\n",
    "    val_losss.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Train loss {trian_loss:.4f} acc {train_acc:.3f} | Val loss {val_loss:.4f} acc {val_acc:.3f}\")\n",
    "\n",
    "    schedular.step(val_loss)\n",
    "\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss=val_loss\n",
    "        best_model_wts=copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(),\"best_custom_cnn.pth\")\n",
    "        print(\"Saved best Model\")\n",
    "        early_stop_counter=0\n",
    "    else :\n",
    "        early_stop_counter+=1\n",
    "        if early_stop_counter>=patience:\n",
    "            print(\"Early Stopping\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76538f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
